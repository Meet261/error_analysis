{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f451a90-0c2a-4e5d-be49-7b9a9e67d474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 16/16 [00:23<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🗂 Unique dependency files found in current batch:\n",
      "\n",
      "🔗 Project URL: https://osf.io/zhf98/\n",
      "📁 Project ID: zhf98\n",
      "📄 Files Found: []\n",
      "\n",
      "🔗 Project URL: https://osf.io/dkq3f/\n",
      "📁 Project ID: dkq3f\n",
      "📄 Files Found: ['README.txt']\n",
      "\n",
      "🔗 Project URL: https://osf.io/w97h4/\n",
      "📁 Project ID: w97h4\n",
      "📄 Files Found: []\n",
      "\n",
      "🔗 Project URL: https://osf.io/7z3mk/\n",
      "📁 Project ID: 7z3mk\n",
      "📄 Files Found: []\n",
      "\n",
      "🔗 Project URL: https://osf.io/ajf3h/\n",
      "📁 Project ID: ajf3h\n",
      "📄 Files Found: []\n",
      "\n",
      "🔗 Project URL: https://osf.io/9mc84/\n",
      "📁 Project ID: 9mc84\n",
      "📄 Files Found: []\n",
      "\n",
      "🔗 Project URL: https://osf.io/5xdbu/\n",
      "📁 Project ID: 5xdbu\n",
      "📄 Files Found: []\n",
      "\n",
      "🔗 Project URL: https://osf.io/p2xgq/\n",
      "📁 Project ID: p2xgq\n",
      "📄 Files Found: []\n",
      "\n",
      "🔗 Project URL: https://osf.io/n7sep/\n",
      "📁 Project ID: n7sep\n",
      "📄 Files Found: []\n",
      "\n",
      "🔗 Project URL: https://osf.io/kgtx6/\n",
      "📁 Project ID: kgtx6\n",
      "📄 Files Found: []\n",
      "\n",
      "🔗 Project URL: https://osf.io/drv3a/\n",
      "📁 Project ID: drv3a\n",
      "📄 Files Found: []\n",
      "\n",
      "🔗 Project URL: https://osf.io/b4gc7/\n",
      "📁 Project ID: b4gc7\n",
      "📄 Files Found: []\n",
      "\n",
      "🔗 Project URL: https://osf.io/6pq9e/\n",
      "📁 Project ID: 6pq9e\n",
      "📄 Files Found: []\n",
      "\n",
      "🔗 Project URL: https://osf.io/r24vb/\n",
      "📁 Project ID: r24vb\n",
      "📄 Files Found: []\n",
      "\n",
      "🔗 Project URL: https://osf.io/mc26t/\n",
      "📁 Project ID: mc26t\n",
      "📄 Files Found: []\n",
      "\n",
      "🔗 Project URL: https://osf.io/6krj7/\n",
      "📁 Project ID: 6krj7\n",
      "📄 Files Found: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from osfclient.api import OSF\n",
    "\n",
    "# Set your batch range here manually\n",
    "start_idx = 280\n",
    "end_idx = 300  # Example: process URL 0 to 9\n",
    "\n",
    "# Define dependency files to search for\n",
    "dependency_files = [\n",
    "    'renv.lock', 'sessionInfo.txt', 'sessionInfo.RData', '.Rprofile', 'DESCRIPTION',\n",
    "    'NAMESPACE', 'requirements.txt', 'environment.yml', 'Dockerfile', 'README.md',\n",
    "    'README.txt', 'Makefile', 'metadata.yml', 'metadata.json', 'dependencies.R', 'dependency.R'\n",
    "]\n",
    "dependency_files_set = set(dependency_files)\n",
    "\n",
    "# Load your OSF metadata\n",
    "osf_metadata = pd.read_csv(\"StatCodeSearch-Code-Comment.csv\")\n",
    "source_url_list = list(osf_metadata.Source.dropna().unique())\n",
    "batch_urls = source_url_list[start_idx:end_idx]\n",
    "\n",
    "# Initialize OSF client and cache\n",
    "osf = OSF()\n",
    "file_cache = {}\n",
    "\n",
    "# Search function\n",
    "def search_dependency_files(storage, dependency_files_set):\n",
    "    unique_files = []\n",
    "    try:\n",
    "        for file in storage.files:\n",
    "            if file.name in dependency_files_set:\n",
    "                unique_files.append(file.name)\n",
    "                if len(unique_files) == len(dependency_files_set):\n",
    "                    return unique_files\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing storage: {e}\")\n",
    "    return unique_files\n",
    "\n",
    "# Fetch files for one project\n",
    "def fetch_unique_files(url):\n",
    "    project_id = url.strip('/').split('/')[-1]\n",
    "    retries = 5\n",
    "    delay = 3\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            if project_id in file_cache:\n",
    "                return url, project_id, file_cache[project_id]\n",
    "            \n",
    "            project = osf.project(project_id)\n",
    "            storage = project.storage('osfstorage')\n",
    "            unique_files = search_dependency_files(storage, dependency_files_set)\n",
    "\n",
    "            file_cache[project_id] = unique_files\n",
    "            return url, project_id, unique_files\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e) and attempt < retries - 1:\n",
    "                print(f\"⚠️ Rate limited on project {project_id}, retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2\n",
    "            else:\n",
    "                print(f\"❌ Error processing project {project_id}: {e}\")\n",
    "                return url, project_id, []\n",
    "\n",
    "# Run the batch\n",
    "all_unique_files = {}\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = {executor.submit(fetch_unique_files, url): url for url in batch_urls}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), mininterval=0.5):\n",
    "        try:\n",
    "            url, project_id, unique_files = future.result()\n",
    "            all_unique_files[url] = {\n",
    "                \"project_id\": project_id,\n",
    "                \"files\": unique_files\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving result: {e}\")\n",
    "\n",
    "# Print result\n",
    "print(\"\\n🗂 Unique dependency files found in current batch:\")\n",
    "for url, details in all_unique_files.items():\n",
    "    print(f\"\\n🔗 Project URL: {url}\")\n",
    "    print(f\"📁 Project ID: {details['project_id']}\")\n",
    "    print(\"📄 Files Found:\", details[\"files\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6179bb4-3805-4a97-9281-9403f9cf386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "output_file = \"osf_dependency_result.csv\"\n",
    "\n",
    "# Check if the file exists to avoid writing the header again\n",
    "write_header = not os.path.exists(output_file)\n",
    "\n",
    "with open(output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    if write_header:\n",
    "        writer.writerow([\"Project URL\", \"Project ID\", \"Files Found\"])\n",
    "\n",
    "    for url, details in all_unique_files.items():\n",
    "        project_id = details.get(\"project_id\", \"N/A\")\n",
    "        files = \", \".join(details.get(\"files\", []))\n",
    "        writer.writerow([url, project_id, files])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "96663ea1-1e7c-4d71-b5bd-759f0f271927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Unique Dependency File Counts:\n",
      "README.txt: 17\n",
      "README.md: 13\n",
      "DESCRIPTION: 2\n",
      "NAMESPACE: 2\n",
      "Makefile: 2\n",
      "Dockerfile: 1\n",
      "sessionInfo.txt: 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# List of dependencies\n",
    "dependency_files = [\n",
    "    'renv.lock', 'sessionInfo.txt', 'sessionInfo.RData', '.Rprofile', 'DESCRIPTION',\n",
    "    'NAMESPACE', 'requirements.txt', 'environment.yml', 'Dockerfile', 'README.md',\n",
    "    'README.txt', 'Makefile', 'metadata.yml', 'metadata.json', 'dependencies.R', 'dependency.R'\n",
    "]\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"osf_dependency_result.csv\")  # Replace with your actual file path\n",
    "\n",
    "# Extract and split all found files\n",
    "all_found_files = []\n",
    "\n",
    "for entry in df[\"Files Found\"].dropna():\n",
    "    files = [f.strip() for f in entry.split(\",\")]\n",
    "    all_found_files.extend(files)\n",
    "\n",
    "# Count only those in dependency_files\n",
    "filtered_counts = Counter(f for f in all_found_files if f in dependency_files)\n",
    "\n",
    "# Display results\n",
    "print(\"📊 Unique Dependency File Counts:\")\n",
    "for dep_file, count in filtered_counts.items():\n",
    "    print(f\"{dep_file}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12adc03d-5bc6-4626-b270-4d6600a56ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "32001d03-2848-437f-98c8-b344847a6b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Dependency File Usage Across Projects:\n",
      "\n",
      "DESCRIPTION (2 projects): w7pjy, vguey\n",
      "Dockerfile (1 projects): 3uyjt\n",
      "Makefile (2 projects): fb5tw, 3uyjt\n",
      "NAMESPACE (2 projects): w7pjy, vguey\n",
      "README.md (13 projects): 7h94n, emwgp, 5y27d, nd9yr, 2j47e, fb5tw, 7mey8, uygpq, 3fnjq, zh3f4, f6qsk, vguey, csy8q\n",
      "README.txt (17 projects): 6ukwg, wbyj7, dqjyh, zcv4m, k853j, 9vr6q, rmcuy, 3wy58, 67ncp, 67ncp, cxv5k, 9jxzs, ex9fj, zh3f4, c8vfj, dez9b, dkq3f\n",
      "sessionInfo.txt (1 projects): cqsr8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# List of known dependency files\n",
    "dependency_files = [\n",
    "    'renv.lock', 'sessionInfo.txt', 'sessionInfo.RData', '.Rprofile', 'DESCRIPTION',\n",
    "    'NAMESPACE', 'requirements.txt', 'environment.yml', 'Dockerfile', 'README.md',\n",
    "    'README.txt', 'Makefile', 'metadata.yml', 'metadata.json', 'dependencies.R', 'dependency.R'\n",
    "]\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(\"osf_dependency_result.csv\")  # Replace with your actual file name\n",
    "\n",
    "# Prepare storage for file-to-project mapping\n",
    "file_to_projects = defaultdict(list)\n",
    "\n",
    "# Process each row\n",
    "for _, row in df.iterrows():\n",
    "    project_id = row[\"Project ID\"]\n",
    "    files = str(row[\"Files Found\"]).split(\",\") if pd.notna(row[\"Files Found\"]) else []\n",
    "\n",
    "    for f in files:\n",
    "        f = f.strip()\n",
    "        if f in dependency_files:\n",
    "            file_to_projects[f].append(project_id)\n",
    "\n",
    "# Print results\n",
    "print(\"📦 Dependency File Usage Across Projects:\\n\")\n",
    "for f in sorted(file_to_projects.keys()):\n",
    "    projects = file_to_projects[f]\n",
    "    print(f\"{f} ({len(projects)} projects): {', '.join(projects)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f4c7be7-494c-4575-928d-2b6a1ae01074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filtered results saved to 'filtered_dependency_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your metadata file with source URLs\n",
    "osf_metadata = pd.read_csv(\"StatCodeSearch-Code-Comment.csv\")\n",
    "source_url_list = list(osf_metadata[\"Source\"].dropna().unique())\n",
    "\n",
    "# Load the results file\n",
    "results_df = pd.read_csv(\"osf_dependency_results.csv\")  # Replace with your actual file path\n",
    "\n",
    "# Filter the results based on matching URLs from source_url_list\n",
    "filtered_results = results_df[results_df[\"Project URL\"].isin(source_url_list)].copy()\n",
    "\n",
    "# Save the filtered results to a new CSV\n",
    "filtered_results.to_csv(\"filtered_dependency_results.csv\", index=False)\n",
    "\n",
    "print(\"✅ Filtered results saved to 'filtered_dependency_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d5be54e-5131-4146-8c01-9496defbf2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Sample of 5 missing Project URLs:\n",
      "['https://osf.io/qxf5t/', 'https://osf.io/m6pb2/', 'https://osf.io/wxgzu/', 'https://osf.io/u3wby/', 'https://osf.io/3w8eg/']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load main metadata file\n",
    "osf_metadata = pd.read_csv(\"StatCodeSearch-Code-Comment.csv\")\n",
    "source_url_list = set(osf_metadata[\"Source\"].dropna().unique())\n",
    "\n",
    "# Load results file\n",
    "results_df = pd.read_csv(\"osf_dependency_results.csv\")  \n",
    "result_urls = set(results_df[\"Project URL\"].dropna().unique())\n",
    "\n",
    "# Find URLs present in main file but missing in result file\n",
    "missing_urls = list(source_url_list - result_urls)\n",
    "\n",
    "# Show 5 examples\n",
    "print(\"🔍 Sample of 5 missing Project URLs:\")\n",
    "print(missing_urls[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1867e3-b7f8-482f-91a7-ad7c827e58f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
